<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Kafaka搭建 | 云起迎风燕</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafaka搭建</h1><a id="logo" href="/.">云起迎风燕</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kafaka搭建</h1><div class="post-meta">2021-08-11<span> | </span><span class="category"><a href="/categories/Kafaka/">Kafaka</a></span></div><div class="post-content"><h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1.Kafka概述"></a>1.Kafka概述</h1><p>官方网站：<a target="_blank" rel="noopener" href="http://kafka.apache.org/">http://kafka.apache.org</a><br>kafka的目标是实现一个为处理实时数据提供一个统一、高吞吐、低延迟的平台。是分布式发布-订阅消息系统，是一个分布式的，可划分的，冗余备份的持久性的日志服务</p>
<p>简单概念</p>
<ul>
<li>Kafka作为一个集群运行在一个或多个可以跨多个数据中心的服务器上</li>
<li>Kafka集群以**主题(topic)**为类别进行数据存储</li>
<li>每条数据都是由一个键、一个值和一个时间戳组成</li>
</ul>
<h1 id="2-环境搭建"><a href="#2-环境搭建" class="headerlink" title="2.环境搭建"></a>2.环境搭建</h1><h2 id="2-1-预备环境"><a href="#2-1-预备环境" class="headerlink" title="2.1 预备环境"></a>2.1 预备环境</h2><ul>
<li><p>安装Jdk1.8</p>
</li>
<li><p>安装Scala2.10.x</p>
</li>
<li><p>安装Kafka 0.10.x</p>
<p>下载地址：<a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a></p>
</li>
</ul>
<h2 id="2-2-启动"><a href="#2-2-启动" class="headerlink" title="2.2 启动"></a>2.2 启动</h2><p>解压缩成功后就可以启动Kafka服务器了。不过在此之前，首先需要启动Zookeeper服务器(Zookeeper为Kafka提供协调服务的工具)。Kafka内置了一个Zookeeper服务器以及一组相关的管理脚本</p>
<p>启动Zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>

<p>启动Kafaka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p>创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic test --partitions 1 --replication-factor 1</span><br></pre></td></tr></table></figure>

<p>查看topic列表</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>

<p>查看topic状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test</span><br></pre></td></tr></table></figure>

<p>  输出</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Topic:test	PartitionCount:1	ReplicationFactor:1</span><br></pre></td></tr></table></figure>

<p>  topic名为test，分区数为1，副本数为1</p>
<h2 id="2-3-发送消息"><a href="#2-3-发送消息" class="headerlink" title="2.3 发送消息"></a>2.3 发送消息</h2><p>Kafka默认提供了脚本工具可以不断接收标准输入，并将它们发送到Kafka的某个<strong>topic</strong>上</p>
<p>用户在控制台终端下启动该命令，输入一行文本数据，该脚本将该行文本封装成一条Kafka消息发送给指定的topic</p>
<p>生产消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br></pre></td></tr></table></figure>

<p>消费消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure>

<h2 id="2-4-创建多broker集群"><a href="#2-4-创建多broker集群" class="headerlink" title="2.4 创建多broker集群"></a>2.4 创建多broker集群</h2><p>为每个broker创建独立的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp config/server.properties config/server-1.properties</span><br><span class="line">cp config/server.properties config/server-2.properties</span><br></pre></td></tr></table></figure>

<p>修改配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config/server-1.properties:</span><br><span class="line">    broker.id=1</span><br><span class="line">    listeners=PLAINTEXT://:9093</span><br><span class="line">    log.dir=/tmp/kafka-logs-1</span><br><span class="line"></span><br><span class="line">config/server-2.properties:</span><br><span class="line">    broker.id=2</span><br><span class="line">    listeners=PLAINTEXT://:9094</span><br><span class="line">    log.dir=/tmp/kafka-logs-2</span><br></pre></td></tr></table></figure>

<p>启动多个broker</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server-1.properties</span><br><span class="line">bin/kafka-server-start.sh config/server-2.properties</span><br></pre></td></tr></table></figure>

<p>创建新的topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<p>查看topic状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<p>生产消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<p>消费消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<h1 id="3-Kafka架构设计"><a href="#3-Kafka架构设计" class="headerlink" title="3.Kafka架构设计"></a>3.Kafka架构设计</h1><h2 id="3-1-Kafka设计目标"><a href="#3-1-Kafka设计目标" class="headerlink" title="3.1 Kafka设计目标"></a>3.1 Kafka设计目标</h2><p>Kafka设计的初衷是为了解决互联网公司超大量级数据的实时传输。为了实现这个目标，Kafka在设计之初就需要考虑以下4个方面的问题</p>
<ul>
<li>吞吐量/延时</li>
<li>消息持久化</li>
<li>负载均衡和故障转移</li>
<li>伸缩性</li>
</ul>
<p><strong>吞吐量/延时</strong></p>
<p><font color=red>吞吐量是某种处理能力的最大值</font>。对于Kafka而言，它的吞吐量就是每秒能够处理的消息数或每秒能够处理的字节数。很显然，吞吐量越大越好</p>
<p><font color=red>延时是衡量一段时间间隔，例如一次请求到响应之间的时间</font>。对于Kafka而言，延时表示客户端发起请求与服务器处理请求并发送响应给客户端之间的这一段时间。显而易见，延时间隔越短越好</p>
<p>在实际使用场景下，这两个指标通常是一对矛盾体，即调优其中一个指标通常会使另一个指标变差</p>
<p><strong>如何提高吞吐量，降低延时</strong></p>
<p>Kafka的写入操作很快，这得益于它对磁盘的使用方法。Kafka会持久化所有数据到磁盘，但每次的写入操作实际都是把数据写入到操作系统的页缓存(page cache)中。由操作系统决定何时把页缓存中的数据写到磁盘上。</p>
<p>设计优势：</p>
<ul>
<li>页缓存是在内存中进行分配，写入操作非常快</li>
<li>无需直接与底层文件系统打交道，由操作系统进行处理</li>
<li>使用追加写入方式进行数据写入，避免磁盘随机写操作<br><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4x4osg43aj30ww0buq3r.jpg"></li>
</ul>
<p>普通的物理磁盘(非固态硬盘)而言，随机读/写确实很慢，但顺序读/写的操作其实非常快，它的速度甚至可以匹敌内存的随机读写速度。鉴于这一事实，Kafka在设计时采用<font color=red>追加写入</font>消息的方式，即只能在日志文件末尾追加写入新的消息，且<font color=red>不允许修改已写入的消息</font></p>
<p><strong>“零拷贝”</strong></p>
<p>在写一个服务端程序时（Web Server或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：<strong>将服务端主机磁盘中的文件不做修改地从已连接的socket发出去</strong>，我们通常用下面的代码完成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>((n = read(diskfd, buf, BUF_SIZE)) &gt; <span class="number">0</span>)</span><br><span class="line">    write(sockfd, buf , n);</span><br></pre></td></tr></table></figure>

<p>基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到<code>socket</code>。但是由于Linux的<code>I/O</code>操作默认是缓冲<code>I/O</code>。这里面主要使用的也就是<code>read</code>和<code>write</code>两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上<code>I/O</code>操作中，发生了多次的数据拷贝</p>
<p>当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据<code>read</code>系统调用提供的<code>buf</code>地址，将内核缓冲区的内容拷贝到<code>buf</code>所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠<code>DMA</code>来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。<br>接下来，<code>write</code>系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后<code>socket</code>再把内核缓冲区的内容发送到网卡上</p>
<img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4x53rljm6j30tw0k6q5e.jpg" width=500 />

<p>从上图中可以看出，共产生了四次数据拷贝，即使使用了<code>DMA</code>来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担</p>
<p><font color=red>在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性</font></p>
<p>零拷贝主要的任务就是<strong>避免</strong>CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务</p>
<p><strong>使用mmap</strong></p>
<p>我们减少拷贝次数的一种方法是调用mmap()来代替read调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">buf = mmap(diskfd, <span class="built_in">len</span>);</span><br><span class="line">write(sockfd, buf, <span class="built_in">len</span>);</span><br></pre></td></tr></table></figure>

<p>用程序调用<code>mmap()</code>，磁盘上的数据会通过<code>DMA</code>被拷贝到内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用<code>write()</code>,操作系统直接将内核缓冲区的内容拷贝到<code>socket</code>缓冲区中，这一切都发生在内核态，最后，<code>socket</code>缓冲区再把数据发到网卡去</p>
<img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4x5z8wqc9j30uc0lsjuc.jpg" width=400 />

<p><strong>使用sendfile</strong></p>
<p>从2.1版内核开始，Linux引入了<code>sendfile</code>来简化操作</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/sendfile.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">ssize_t</span> <span class="title">sendfile</span><span class="params">(<span class="keyword">int</span> out_fd, <span class="keyword">int</span> in_fd, <span class="keyword">off_t</span> *offset, <span class="keyword">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure>

<p>系统调用<code>sendfile()</code>在代表输入文件的描述符<code>in_fd</code>和代表输出文件的描述符<code>out_fd</code>之间传送文件内容(字节)</p>
<p>使用<code>sendfile</code>不仅减少了数据拷贝的次数，还减少了上下文切换，数据传送始终只发生在<code>kernel space</code></p>
<img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4x61mkd5rj30ya0iumyy.jpg" width=500 />

<p>Kafka的消息消费机制使用的就是<code>sendfile()</code>，严格来说就是通过Java的<code>FileChannel.transferTo</code>方法实现</p>
<p>除了”零拷贝”技术，Kafka大量使用页缓存，因此读取消息时大部分消息很有可能依然保存在也缓存中。因此可以直接命中缓存，不用”穿透”到底层的物理磁盘上获取信息</p>
<p>总结，Kafka依靠下列4点达到高吞吐量、低延时的设计目标</p>
<ul>
<li>大量使用操作系统页缓存，内存操作速度快且命中率高</li>
<li>Kafka不直接参与物理I/O操作，交由操作系统完成</li>
<li>采用追加写入方式，摒弃缓慢的磁盘随机读/写操作</li>
<li>使用以<code>sendfile</code>为代表的零拷贝加强网络间的数据传输效率</li>
</ul>
<p><strong>消息持久化</strong></p>
<p>Kafka对消息进行了持久化操作，这样做的好处如下</p>
<ul>
<li>解耦消息发送与消息消费</li>
<li>实现灵活的消息处理</li>
</ul>
<h2 id="3-2-消息"><a href="#3-2-消息" class="headerlink" title="3.2 消息"></a>3.2 消息</h2><p>Kafka的消息是由一个固定长度的消息头+可变长的key字节数组+可变长的vaue字节数组组成</p>
<p>消息头由以下几部分组成</p>
<ul>
<li>4个字节用于检测消息完整性的CRC校验和</li>
<li>1个字节的版本号</li>
<li>1个字节的消息属性</li>
<li>8个字节的时间戳</li>
</ul>
<p>完整的消息结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">* 1. 4 byte CRC32 of the message</span><br><span class="line">* 2. 1 byte &quot;magic&quot; identifier to allow format changes, value is 0 or 1</span><br><span class="line">* 3. 1 byte &quot;attributes&quot; identifier to allow annotations on the message independent of the version</span><br><span class="line">*    bit 0 ~ 2 : Compression codec.</span><br><span class="line">*      0 : no compression</span><br><span class="line">*      1 : gzip</span><br><span class="line">*      2 : snappy</span><br><span class="line">*      3 : lz4</span><br><span class="line">*    bit 3 : Timestamp type</span><br><span class="line">*      0 : create time</span><br><span class="line">*      1 : log append time</span><br><span class="line">*    bit 4 ~ 7 : reserved</span><br><span class="line">* 4. (Optional) 8 byte timestamp only if &quot;magic&quot; identifier is greater than 0</span><br><span class="line">* 5. 4 byte key length, containing length K</span><br><span class="line">* 6. K byte key</span><br><span class="line">* 7. 4 byte payload length, containing length V</span><br><span class="line">* 8. V byte payload</span><br><span class="line">*/</span><br></pre></td></tr></table></figure>

<img src="http://ww4.sinaimg.cn/large/006tNc79gy1g533pw7xm3j30ww0ec75f.jpg" width=600 />

<h2 id="3-2-主题与分区"><a href="#3-2-主题与分区" class="headerlink" title="3.2 主题与分区"></a>3.2 主题与分区</h2><ul>
<li><p>主题(Topic)</p>
<p><strong>topic</strong>只是一个逻辑上的概念，它表示某种同类型消息的集合。也可以认为是消息被发送到的地方。通常我们可以通过<strong>topic</strong>来区分不同的业务，比如业务A使用一个<strong>topic</strong>，业务B使用另外一个<strong>topic</strong></p>
<p>Kafka中的<strong>topic</strong>通常会被多个消费者订阅，出于性能的考量，Kafka并不是<font color=red>topic-message</font>，而是<font color=red>topic-partition-message</font>的三层结构来分散负载。从本质上来说，每个<strong>topic</strong>都是由多个<strong>partition</strong>组成</p>
<img src="http://ww4.sinaimg.cn/large/006tNc79gy1g534g3li5aj30mu0dsaat.jpg" width=480 /></li>
<li><p>分区(Partition)</p>
<p>每个分区都是一个有序的、不可变的记录序列，这些记录连续附加到一个结构化提交日志中。每个分区中的记录都被分配一个称为偏移量的顺序ID号，该偏移量唯一地标识分区中的每个记录</p>
<p>Kafka中的<strong>partition</strong>实际上并没有太多的业务含义，它的引入只是单纯为了提升系统的吞吐量。因此在创建<strong>topic</strong>时，可以根据集群实际配置设置具体的<strong>partition</strong>数，实现整体性能的最大化</p>
</li>
</ul>
<h2 id="3-3-offset"><a href="#3-3-offset" class="headerlink" title="3.3 offset"></a>3.3 offset</h2><p>在Kafka的消费端有着偏移量(offset)的概念</p>
<img src="http://ww4.sinaimg.cn/large/006tNc79gy1g534pdj8m8j30li0csq3k.jpg" width=400 />

<p>显然，每条消息在某个partition上的位移是固定的，但消费该partition的消费者的位移会随着消费进度不断前移，但终究不会超过该分区最新一条消息的偏移量</p>
<p><font color=red>因此，对于Kafka中的任意一条消息，都可以表示成&lt;topic,partition,offset&gt;三元组，通过该元组来定位到唯一一条消息</font></p>
<h2 id="3-4-replica"><a href="#3-4-replica" class="headerlink" title="3.4 replica"></a>3.4 replica</h2><p>对于partition中的有序日志消息，为了保证的数据的有效性，以及系统的可靠性，数据本身是需要进行冗余操作的。这些冗余备份的日志消息，在Kafka中被称为副本(replica)，它们存在的唯一目的就是防止数据丢失。</p>
<p>副本分为两类</p>
<ul>
<li>领导者副本(leader replica)</li>
<li>跟随者副本(follower replica)</li>
</ul>
<p><strong>leader</strong></p>
<p>​    leader对外提供服务</p>
<p><strong>follower</strong></p>
<p>​    被动向leader获取数据，如果leader所在的broker出现宕机，Kafka就会从剩余的follower中选举出新的leader</p>
<h2 id="3-5-ISR"><a href="#3-5-ISR" class="headerlink" title="3.5 ISR"></a>3.5 ISR</h2><p>ISR(in-sync replica)，表示与leader replica保持同步的replica集合</p>
<p>Kafka为parition动态维护一个replica集合，该集合中的所有replica保存的消息日志都与leader replica保持同步状态。<font color=red>只有这个集合中的replica才会被选举为leader，也只有集合中所有replica都接收到同一消息，Kafka才会将该消息置为”已提交”状态</font>，即认为这条消息发送成功</p>
<p>正常情况下，partition中所有的replica都应该与leader replica保持同步，即所有的replica都在ISR中。由于各种各样的原因，一小部分的replica开始落后与leader replica，当滞后到一定程度时，Kafka会将这些replica”踢”出ISR。当这些replica重新”追上”leader的进度时，Kafka会将他们重新加回到ISR中</p>
<p>这一些都是自动维护的，不需要人工干预</p>
<h1 id="4-Kafka集群搭建"><a href="#4-Kafka集群搭建" class="headerlink" title="4.Kafka集群搭建"></a>4.Kafka集群搭建</h1><h2 id="4-1-搭建Zookeeper集群"><a href="#4-1-搭建Zookeeper集群" class="headerlink" title="4.1 搭建Zookeeper集群"></a>4.1 搭建Zookeeper集群</h2><p><strong>主机配置</strong></p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>部署模块</th>
</tr>
</thead>
<tbody><tr>
<td>master</td>
<td>zookeeper、kafka</td>
</tr>
<tr>
<td>node1</td>
<td>zookeeper、kafka</td>
</tr>
<tr>
<td>node2</td>
<td>zookeeper、kafka</td>
</tr>
</tbody></table>
<p><strong>Zookeeper集群搭建</strong></p>
<ul>
<li><p>解压</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxf zookeeper-3.4.5.tar.gz -C /opt/modules</span><br></pre></td></tr></table></figure></li>
<li><p>复制配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp conf/zoo_sample.cfg conf/zoo.cfg</span><br></pre></td></tr></table></figure></li>
<li><p>配置数据存储目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/modules/zookeeper-3.4.5/data</span><br></pre></td></tr></table></figure></li>
<li><p>创建数据存储目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /opt/modules/zookeeper-3.4.5/data</span><br></pre></td></tr></table></figure></li>
<li><p>配置集群节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi conf/zoo.cfg</span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure></li>
<li><p>在数据存储目录下创建名为myid的文件，内容为节点对应id</p>
</li>
<li><p>启动每台节点的zookeeper</p>
</li>
</ul>
<h2 id="4-2-搭建Kafka集群"><a href="#4-2-搭建Kafka集群" class="headerlink" title="4.2 搭建Kafka集群"></a>4.2 搭建Kafka集群</h2><ul>
<li><p>编写配置文件(server.properties)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0   #0/1/2</span><br><span class="line">listeners=PLAINTEXT://master:9092  #node1:9092/node2:9092</span><br><span class="line">zookeeper.connect=master:2181,master:2181,master:2181</span><br></pre></td></tr></table></figure></li>
<li><p>启动kafka server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="5-Producer-API"><a href="#5-Producer-API" class="headerlink" title="5.Producer API"></a>5.Producer API</h1><p>producer的主要功能就是向某个topic的某个分区发送消息，所以它首先需要确认想topic的哪个分区写入消息</p>
<p>分区的确认是由分区器(partitioner)完成</p>
<h2 id="5-1-producer实现"><a href="#5-1-producer实现" class="headerlink" title="5.1 producer实现"></a>5.1 producer实现</h2><p>添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>代码实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.12:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">0</span>);</span><br><span class="line">props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line">props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"></span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>

<p>生产者包含一个缓冲区，其中保存尚未传输到服务器的记录，以及一个后台I/O线程，该线程负责将这些记录转换为请求并将它们传输到集群。使用后不关闭生产商将泄漏这些资源</p>
<p><code>send()</code>方法是异步的。调用该方法时，它将记录发送到缓冲区，并立即返回。通过这种方式来以批处理的方式发送数据，从而提高效率</p>
<p>生产者为每个分区维护未发送记录的缓冲区。这些缓冲区的大小由batch.size配置指定。增大这个值可能会导致更多的批处理，但需要更多的内存</p>
<p>默认情况下，即使缓冲区中有额外的未使用空间，也可以立即发送缓冲区。但是，如果要减少请求数，可以将linger.ms设置为大于0的值。这将指示生产者在发送请求之前等待最长的毫秒数，以希望有更多的记录到达以填充同一批</p>
<p>获取返回结果</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.12:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">0</span>);</span><br><span class="line">props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line">props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, Integer.toString(i), Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;send success&quot;</span>);</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;send failure&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>

<h2 id="5-2-参数说明"><a href="#5-2-参数说明" class="headerlink" title="5.2 参数说明"></a>5.2 参数说明</h2><table>
<thead>
<tr>
<th>参数名</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>用于建立与Kafka群集的初始连接的主机/端口对列表。此列表的格式应为host1:port1,host1:port2,…</td>
</tr>
<tr>
<td>key.serializer</td>
<td>设置key的序列化实现类</td>
</tr>
<tr>
<td>value.serializer</td>
<td>设置value的序列化实现类</td>
</tr>
<tr>
<td>acks</td>
<td>指定响应producer请求前，写入该消息的副本数。0表示producer不用关心leader端的处理结果，all表示leader以及ISR中所有副本写入完成后返回响应结果，1为0和all的折中方案，也是默认参数值，表示leader broker仅将消息写入本地日志后，就会producer进行相应</td>
</tr>
<tr>
<td>buffer.memory</td>
<td>指定producer端用于缓存消息的缓冲区大小，默认32M</td>
</tr>
<tr>
<td>compression.type</td>
<td>默认情况下，消息发送时不会被压缩。该参数可以设置为 snappy、gzip 或 lz4，它指定了消息被发送给 broker 之前使用哪一种压缩算法进行压缩                                  1.snappy 压缩算法由 Google 发明，它占用较少的 CPU，却能提供较好的性能和相当可观的压缩比，如果比较关注性能和网络带宽，可以使用这种算法                   2.gzip 压缩算法一般会占用较多的 CPU，但会提供更高的压缩比，所以如果网络带宽比较有限，可以使用这种算法</td>
</tr>
<tr>
<td>retries</td>
<td>指定在写入消息出现异常时，进行写入重试的次数，默认为0</td>
</tr>
<tr>
<td>batch.size</td>
<td>当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算(而不是消息个数)。批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定都会等到批次被填满才发送，半满的批次，甚至只包含一个消息的批次也有可能被发送。所以就算把批次大小设置得很大，也不会造成延迟，只是会占用更多的内存而已。但如果设置得太小，因为生产者需要更频繁地发送消息，会增加一些额外的开销</td>
</tr>
<tr>
<td>linger.ms</td>
<td>该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。KafkaProducer 会在批次填满或 linger.ms 达到上限时把批次发送出去。默认情况下，只要有可用的线程，就算批次里只有一个消息，生产者也会把消息发送出去。把 linger.ms 设置成比 0 大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次。虽然这样会增加延迟，但也会提升吞吐量(因为一次性发送更多的消息，每个消息的开销就变小了)</td>
</tr>
<tr>
<td>max.request.size</td>
<td>控制producer发送请求的大小，实际是控制producer端能够发送的最大消息大小，默认为1048576字节，通常无法满足实际需求</td>
</tr>
<tr>
<td>request.timeout.ms</td>
<td>控制producer发送后的等待响应时间，默认为30s</td>
</tr>
</tbody></table>
<h2 id="5-3-消息分区策略"><a href="#5-3-消息分区策略" class="headerlink" title="5.3 消息分区策略"></a>5.3 消息分区策略</h2><p>producer在发送消息过程中，最重要的一步就是确定将消息发送到topic的哪个partition中。Kafka提供的分区器(partitioner)会尽力确保相同key的所有消息都发送到相同的分区上，若没有指定key，则会使用轮询的方式来确保消息在topic上是均匀分配的</p>
<h2 id="5-4-自定义分区"><a href="#5-4-自定义分区" class="headerlink" title="5.4 自定义分区"></a>5.4 自定义分区</h2><p>实现Partitioner接口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Random random;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object o, <span class="keyword">byte</span>[] bytes, Object o1, <span class="keyword">byte</span>[] bytes1, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        String key = (String) o1;</span><br><span class="line">        <span class="comment">//获取topic的所有分区信息</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitionInfos = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">        <span class="comment">//随机获取分区</span></span><br><span class="line">        <span class="keyword">int</span> partition = random.nextInt(partitionInfos.size());</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; map)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//该方法用于实现资源的初始化</span></span><br><span class="line">        random = <span class="keyword">new</span> Random();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>添加自定义分区类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.12:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">0</span>);</span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        <span class="comment">//设置定义分区</span></span><br><span class="line">        props.put(<span class="string">&quot;partitioner.class&quot;</span>, <span class="string">&quot;MyPartitioner&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, Integer.toString(i), 	Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span>(e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;send success&quot;</span>);</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;send failure&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>查看分区消息数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic test</span><br></pre></td></tr></table></figure>

<h2 id="5-5-消息序列化"><a href="#5-5-消息序列化" class="headerlink" title="5.5 消息序列化"></a>5.5 消息序列化</h2><table>
<thead>
<tr>
<th>序列化类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>ByteArraySerializer</td>
<td>本质上什么都不用做，因为己经是字节数组</td>
</tr>
<tr>
<td>ByteBufferSerializer</td>
<td>序列化 ByteBuffer</td>
</tr>
<tr>
<td>BytesSerializer</td>
<td>序列化 Kafka 自定义的 Bytes 类</td>
</tr>
<tr>
<td>DoubleSerializer</td>
<td>序列化 Double类型</td>
</tr>
<tr>
<td>IntegerSerializer</td>
<td>序列化 Integer类型</td>
</tr>
<tr>
<td>LongSerializer</td>
<td>序列化 Long类型</td>
</tr>
<tr>
<td>StringSerializer</td>
<td>序列化 String类型</td>
</tr>
</tbody></table>
<h2 id="5-6-producer拦截器"><a href="#5-6-producer拦截器" class="headerlink" title="5.6 producer拦截器"></a>5.6 producer拦截器</h2><p>拦截器 (interceptor)是在Kafka 0.10.0.0版本中被引入，主要用于实现producer和consumer端的定制化控制逻辑</p>
<p>对于 producer 而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer 允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链( interceptor chain ) 。 intercetpor 的实现接口是<code>org.apache.kafka.clients.producer.Producerlnterceptor</code></p>
<h1 id="6-Consumer-API"><a href="#6-Consumer-API" class="headerlink" title="6.Consumer API"></a>6.Consumer API</h1><p>Kafka消费者(consumer)是从 Kafka读取数据的应用，若干个 consumer订阅 Kafka集群中的若干个topic并从 Kafka接收属于这些topic的消息</p>
<p>我们可以讲consumer分为两类</p>
<ul>
<li>独立消费者(standalone consumer)</li>
<li>消费者组(consumer group)</li>
</ul>
<h2 id="6-1-消费者组"><a href="#6-1-消费者组" class="headerlink" title="6.1 消费者组"></a>6.1 消费者组</h2><p>消费者使用一个消费者组名(即 group.id)来标记自己，topic 的每条消息都只会被发送到<font color=red>每个订阅它的消费者组的一个消费者实例上</font></p>
<p>通过消费者组，可以实现对两种模型的支持</p>
<ul>
<li><p>消息队列</p>
<p>所有的consumer都属于同一个group，这样接收到的消息只能被其中一个consumer实例接收</p>
</li>
<li><p>发布/订阅</p>
<p>每个consumer都属于不同的group，这些Kafka消息就会被广播到所有的consumer实例上</p>
</li>
</ul>
<h2 id="6-2-consumer实现"><a href="#6-2-consumer实现" class="headerlink" title="6.2 consumer实现"></a>6.2 consumer实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.12:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">            System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-3-偏移量操作"><a href="#6-3-偏移量操作" class="headerlink" title="6.3 偏移量操作"></a>6.3 偏移量操作</h2><p>Kafka为分区中的每个记录维护一个数字偏移量。此偏移量充当该分区中记录的唯一标识符，还表示使用者在分区中的位置。例如，位于位置5的使用者已使用偏移量为0到4的记录，接下来将接收偏移量为5的记录</p>
<p>实际上，与消费者的用户相关的位置有两个概念：</p>
<ul>
<li>消费者的位置给出了将给出的下一个记录的偏移量，每当消费者调用<code>poll(long)</code>时自动前进</li>
<li>已提交的位置是安全存储的最后一个偏移量，<font color=red>如果进程失败并重新启动，这就是使用者将恢复到的偏移量。使用者可以定期自动提交偏移；也可以选择手动控制提交位置</font></li>
</ul>
<p><strong>自动提交偏移量</strong></p>
<p><font color=red>设置<code>enable.auto.commit</code>意味着自动提交偏移量，其频率由 <code>auto.commit.interval.ms</code>控制</font></p>
<p><strong>手动提交偏移量</strong></p>
<p>用户还可以控制何时应将记录视为已使用记录，从而提交其偏移量，而不是依赖使用者定期提交已使用的偏移量。当消息的消耗与一些处理逻辑相耦合时，这很有用，因此在完成处理之前，不应将消息视为消耗消息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>));</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">200</span>;</span><br><span class="line">List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">     ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">     <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">         buffer.add(record);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">         <span class="comment">//同步到数据库后，手动提交</span></span><br><span class="line">         consumer.commitSync();</span><br><span class="line">         buffer.clear();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为了避免这种情况，只有在相应的记录插入数据库之后，我们才会手动提交偏移量。这使我们能够精确地控制记录何时被视为已消耗。这就产生了相反的可能性：进程可能在插入到数据库之后但在提交之前的时间间隔内失败（尽管这可能只有几毫秒，但还是有可能的）。在这种情况下，接管消耗的进程将从上次提交的偏移量消耗，并重复插入最后一批数据。通过这种方式，<font color=red>Kafka提供了通常称为“至少一次”的交付保证，因为每个记录可能会交付一次，但在失败的情况下可能会出现重复数据</font></p>
<p><strong>提交分区偏移量</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE);</span><br><span class="line">        <span class="keyword">for</span> (TopicPartition partition : records.partitions()) &#123;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123;</span><br><span class="line">                System.out.println(record.offset() + <span class="string">&quot;: &quot;</span> + record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">long</span> lastOffset = partitionRecords.get(partitionRecords.size() - <span class="number">1</span>).offset();</span><br><span class="line">            consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> OffsetAndMetadata(lastOffset + <span class="number">1</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="6-4-偏移量存储外部化"><a href="#6-4-偏移量存储外部化" class="headerlink" title="6.4 偏移量存储外部化"></a>6.4 偏移量存储外部化</h2><p>消费程序可以不适用Kafka内置的偏移量存储，它可以在自己选择的存储中存储偏移量。这种方法主要用于应用程序在同一个系统中同时存储消耗的偏移量和结果，这样结果和偏移量就可以原子地存储</p>
<p><font color=red>如果是这样，它将使消费完全原子化，并提供“恰好一次”语义，这比使用Kafka的偏移提交功能获得的默认“至少一次”语义更强</font></p>
<p>使用场景</p>
<ul>
<li>如果消费的结果存储在关系数据库中，那么可以在数据库中存储偏移量，并将存储结果与偏移量放在同一个事务中</li>
<li>offset也可存储在Zookeeper、HBase、Redis、HDFS、Spark Streaming checkpoint中</li>
</ul>
<h2 id="6-5-控制消费位置"><a href="#6-5-控制消费位置" class="headerlink" title="6.5 控制消费位置"></a>6.5 控制消费位置</h2><p>在大多数用例中，使用者只需从头到尾使用记录，定期提交其位置(自动或手动)。然而，Kafka允许用户手动控制其位置，在分区中随意向前或向后移动。这意味着使用者可以重新使用旧记录，或者跳过最新的记录而不实际使用中间记录</p>
<p>有几种情况下，手动控制消费者的位置是有用的</p>
<ul>
<li>对于时间敏感的记录处理，对于远远落后于处理所有记录的消费者来说，这可能是有意义的，而不仅仅是跳到最近的记录</li>
<li>维护本地状态的系统，在这样的系统中，消费者将希望在启动时将其位置初始化为本地存储中包含的任何内容</li>
</ul>
<p>Kafka允许使用<code>seek(TopicPartition, long)</code>指定位置来指定新位置</p>
<p>还提供了寻找服务器维护的最早和最新偏移量的特殊方法，<code>seekToBeginning(Collection)</code>与<code>seekToEnd(Collection)</code></p>
<h2 id="6-6-参数说明"><a href="#6-6-参数说明" class="headerlink" title="6.6 参数说明"></a>6.6 参数说明</h2><table>
<thead>
<tr>
<th>参数名</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>用于建立与Kafka群集的初始连接的主机/端口对列表。此列表的格式应为host1:port1,host1:port2,…</td>
</tr>
<tr>
<td>key.deserializer</td>
<td>设置key的序列化实现类</td>
</tr>
<tr>
<td>value.deserialize</td>
<td>设置value的序列化实现类</td>
</tr>
<tr>
<td>group.id</td>
<td>指定consumer group的名称，它能够唯一标识一个 consumer group。默认值为空字符串</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>指定consumer是否自动提交位移</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>第一层含义：指定consumer group检测组内成员发送崩溃的时间。第二层含义consumer消息处理逻辑的最大时间，若 consumer 两次poll之间的间隔超过了该参数所设置的阈值，那么coordinator就会认为这个consumer己经追不上组内其他成员的消费进度了，因此会将该consumer实例“踢出”组，该consumer负责的分区也会被分配给其他consumer，默认值为10s</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>单独设置consumer处理逻辑最大时间</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>指定无位移信息或位移越界时Kafka的应对策略。earliest表示从最早的位移开始消费，但未必是从0开始。latest表示从最新处位移开始消费。none表示若未发现位移信息或位移越界，则抛出异常</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>指定consumer端单次获取数据的最大字节数</td>
</tr>
<tr>
<td>connections.max.idle.ms</td>
<td>设置定时关闭consumer与broker的socket连接时间，默认为9分钟，推荐设置为-1</td>
</tr>
</tbody></table>
<h1 id="7-Kafka-Eagle"><a href="#7-Kafka-Eagle" class="headerlink" title="7.Kafka Eagle"></a>7.Kafka Eagle</h1><p>Kafka Eagle是一个监控系统，用于监控你的Kafka集群、可视化消费者线程、offset等</p>
<p>下载地址：<a target="_blank" rel="noopener" href="http://download.kafka-eagle.org/">http://download.kafka-eagle.org</a></p>
<ul>
<li><p>关闭防火墙</p>
</li>
<li><p>```shell<br>systemctl stop firewalld</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 解压kafka-eagle-bin-1.2.3.tar.gz</span><br><span class="line"></span><br><span class="line">- 解压kafka-eagle-web-1.2.3-bin.tar.gz</span><br><span class="line"></span><br><span class="line">- 编辑conf/system-config.properties</span><br><span class="line"></span><br><span class="line">- ```properties</span><br><span class="line">  ######################################</span><br><span class="line">  # multi zookeeper&amp;kafka cluster list</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.zk.cluster.alias=cluster1,cluster2</span><br><span class="line">  cluster1.zk.list=localhost:2181</span><br><span class="line">  cluster2.zk.list=localhost:2181</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # zk client thread limit</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.zk.limit.size=25</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # kafka eagle webui port</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.webui.port=8048</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # kafka offset storage</span><br><span class="line">  ######################################</span><br><span class="line">  cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line">  cluster2.kafka.eagle.offset.storage=zookeeper</span><br><span class="line">  ######################################</span><br><span class="line">  # enable kafka metrics</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.metrics.charts=true</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # alarm email configure</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.mail.enable=true</span><br><span class="line">  kafka.eagle.mail.sa=alert_sa</span><br><span class="line">  kafka.eagle.mail.username=alert_sa@163.com</span><br><span class="line">  kafka.eagle.mail.password=mqslimczkdqabbbg</span><br><span class="line">  kafka.eagle.mail.server.host=smtp.163.com</span><br><span class="line">  kafka.eagle.mail.server.port=25</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # delete kafka topic token</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.topic.token=keadmin</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # kafka sasl authenticate</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.sasl.enable=false</span><br><span class="line">  kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">  kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line">  </span><br><span class="line">  ######################################</span><br><span class="line">  # kafka jdbc driver address</span><br><span class="line">  ######################################</span><br><span class="line">  kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line">  kafka.eagle.url=jdbc:sqlite:/opt/modules/kafka-eagle-bin-1.2.3/kafka-eagle-web-1.2.3/db/ke.db</span><br><span class="line">  kafka.eagle.username=root</span><br><span class="line">  kafka.eagle.password=smartloli</span><br></pre></td></tr></table></figure></li>
<li><p>启动</p>
</li>
<li><pre><code>bin/ke.sh start
</code></pre>
</li>
<li><p>访问：<a href="http://ip:port/ke">http://ip:port/ke</a></p>
</li>
</ul>
</div><div class="tags"><a href="/tags/MySQL/"><i class="fa fa-tag"></i>MySQL</a></div><div class="post-nav"><a class="pre" href="/2021/08/11/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%91%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F/">云存储项目</a><a class="next" href="/2021/08/11/Spark%E4%BC%98%E5%8C%96/">Spark优化</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/FastDFS/">FastDFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JAVA-%E9%A1%B9%E7%9B%AE/">JAVA-项目</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JAVA%E5%9F%BA%E7%A1%80/">JAVA基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JVM/">JVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kafaka/">Kafaka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MyBatis/">MyBatis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Redis/">Redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring-Boot/">Spring Boot</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringMVC/">SpringMVC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafuka/">kafuka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7/">版本控制工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">计算机网络</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/JAVA/" style="font-size: 15px;">JAVA</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/FastDFS/" style="font-size: 15px;">FastDFS</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/JAVA-WEB/" style="font-size: 15px;">JAVA-WEB</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/SVN/" style="font-size: 15px;">SVN</a> <a href="/tags/kafuka/" style="font-size: 15px;">kafuka</a> <a href="/tags/linux%E5%91%BD%E4%BB%A4/" style="font-size: 15px;">linux命令</a> <a href="/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/tags/yum%E5%91%BD%E4%BB%A4/" style="font-size: 15px;">yum命令</a> <a href="/tags/SSL/" style="font-size: 15px;">SSL</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/MyBatis/" style="font-size: 15px;">MyBatis</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Spring-Boot/" style="font-size: 15px;">Spring Boot</a> <a href="/tags/%E9%A1%B9%E7%9B%AE/" style="font-size: 15px;">项目</a> <a href="/tags/SpringMVC/" style="font-size: 15px;">SpringMVC</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/%E5%BF%83%E6%83%85/" style="font-size: 15px;">心情</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2050/08/09/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8F%90%E4%BA%A4/">👋 Hi!</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/kafuka%E6%93%8D%E4%BD%9C/">kafuka操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/CUP%E5%8D%A0%E7%94%A8%E7%8E%87%E8%BF%87%E9%AB%98%E5%AE%9A%E4%BD%8D%E8%BF%87%E7%A8%8B/">定位cup占用过高</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/5.%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/">JAVA集合框架</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/6.%E6%96%87%E4%BB%B6%E4%B8%8EIO/">文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/7.JDBC/">JAVA-JDBC</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/FastDFS%E5%85%A5%E9%97%A8/">FastDFS</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/Git/">Git</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/JavaWeb(%E4%BA%8C)/">Http之HttpServlet</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/Redis%E9%AB%98%E7%BA%A7/">Redis高级</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">云起迎风燕.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>