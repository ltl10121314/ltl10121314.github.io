<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Spark SQL | 云起迎风燕</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark SQL</h1><a id="logo" href="/.">云起迎风燕</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark SQL</h1><div class="post-meta">2021-08-11<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span></div><div class="post-content"><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h1><p>Spark SQL是用于结构化数据处理的Spark模块与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。有几种方法可以与Spark SQL交互，包括SQL、DataFrame API和DataSets API。当计算结果时，使用相同的执行引擎，而与您用来表示计算的API/语言无关。这种统一意味着开发人员可以很容易地在各种API之间来回切换，基于这些API提供了表示给定转换的最自然的方式</p>
<p><strong>为什么需要SQL</strong></p>
<ul>
<li>事实上的标准</li>
<li>易学易用</li>
<li>受众面大</li>
</ul>
<p><strong>hive on shark ==&gt; hive on spark</strong></p>
<img src="http://ww4.sinaimg.cn/large/006tNc79gy1g5eexhlcrbj30lo0eg758.jpg" width=450 />

<p><strong>SQL on Hadoop</strong></p>
<img src="http://ww2.sinaimg.cn/large/006tNc79gy1g5ef2a8qfrj30r00geq3z.jpg" width=500 />

<p><strong>Spark SQL的愿景</strong></p>
<ul>
<li>write less code</li>
<li>read less data</li>
<li>let the optimizer do the hard work</li>
</ul>
<h1 id="2-快速入门"><a href="#2-快速入门" class="headerlink" title="2.快速入门"></a>2.快速入门</h1><h2 id="2-1-SQLContext"><a href="#2-1-SQLContext" class="headerlink" title="2.1 SQLContext"></a>2.1 SQLContext</h2><p>Spark SQL中所有功能的入口点是SQLContext类或其子类之一</p>
<p>要创建一个基本的SQLContext，您只需要一个SparkContext类对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">println(sqlContext)</span><br></pre></td></tr></table></figure>

<p>除了基本的<code>SparkContext</code>之外，还可以创建<code>HiveContext</code>，它提供了基本<code>SparkContext</code>提供的功能的超集。其他功能包括使用更完整的<code>HiveQL</code>解析器编写查询的能力，对<code>Hive UDF</code>的访问，以及从Hive表中读取数据的能力</p>
<p>要使用<code>HiveContext</code>，不需要进行额外的配置，并且所有可用于SQLContext的数据源仍然可用</p>
<p>对于<code>SQLContext</code>，唯一可用的方言是“SQL”，它使用Spark SQL提供的简单SQL解析器。在<code>HiveContext</code>中，默认值是“HiveQL”，尽管“sql”也可用。因为HiveQL解析器更完整，所以建议在大多数用例中使用它</p>
<h2 id="2-2-DataFrames"><a href="#2-2-DataFrames" class="headerlink" title="2.2 DataFrames"></a>2.2 DataFrames</h2><p><code>DataFrame</code>是一个分布式数据集，且内部数据是由多个命名列(列名、列类型、列值)组合而成</p>
<p><code>DataFrame</code>可以看做关系型数据库中的表或R/Python中的<code>DataFrame</code></p>
<p><font color=red>且对<code>DataFrame</code>的执行做了更多的优化</font></p>
<p>使用<code>SQLContext</code>，应用程序可以从现有的RDD、Hive表或数据源创建<code>DataFrame</code></p>
<p><strong>代码演示说明</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">df.show</span><br></pre></td></tr></table></figure>
<p><strong>DataFrame操作</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//显示DataFrame中的内容</span></span><br><span class="line">df.show</span><br><span class="line"></span><br><span class="line"><span class="comment">//以树形结构打印DataFrame数据结构</span></span><br><span class="line">df.printSchema</span><br><span class="line"></span><br><span class="line"><span class="comment">//查询指定列</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>).show</span><br><span class="line"></span><br><span class="line"><span class="comment">//列运算</span></span><br><span class="line">df.select(df(<span class="string">&quot;name&quot;</span>), df(<span class="string">&quot;age&quot;</span>) + <span class="number">1</span>).show</span><br><span class="line"></span><br><span class="line"><span class="comment">//条件过滤</span></span><br><span class="line">df.filter(df(<span class="string">&quot;age&quot;</span>) &gt; <span class="number">21</span>).show</span><br><span class="line"></span><br><span class="line"><span class="comment">//分组统计</span></span><br><span class="line">df.groupBy(<span class="string">&quot;age&quot;</span>).count().show</span><br></pre></td></tr></table></figure>

<h2 id="2-3-DataFrame-vs-RDD"><a href="#2-3-DataFrame-vs-RDD" class="headerlink" title="2.3 DataFrame vs RDD"></a>2.3 DataFrame vs RDD</h2><table>
<thead>
<tr>
<th></th>
<th>DataFrame</th>
<th>RDD</th>
</tr>
</thead>
<tbody><tr>
<td>分布式</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>易用性</td>
<td>高级API，更易用</td>
<td>高阶函数</td>
</tr>
<tr>
<td>通用性</td>
<td>对R、Python使用者无缝集成</td>
<td>需要Scala编程基础</td>
</tr>
<tr>
<td>数据结构</td>
<td>结构化</td>
<td>非结构化</td>
</tr>
</tbody></table>
<img src="http://ww1.sinaimg.cn/large/006tNc79gy1g5efugmps8j31800ig76h.jpg" width=700 />

<h2 id="2-4-SQL查询"><a href="#2-4-SQL查询" class="headerlink" title="2.4 SQL查询"></a>2.4 SQL查询</h2><p><code>SQLContext</code>上的SQL函数使应用程序能够以编程方式运行SQL查询，并以<code>DataFrame</code>的形式返回结果</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = ... <span class="comment">// An existing SQLContext</span></span><br><span class="line"><span class="keyword">val</span> df = sqlContext.sql(<span class="string">&quot;SELECT * FROM table&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="2-5-Datasets"><a href="#2-5-Datasets" class="headerlink" title="2.5 Datasets"></a>2.5 Datasets</h2><p>Datasets与RDDS类似，但并不使用Java序列化或Kryo，而是使用专门的编码器来序列化对象并通过网络进行处理或传输</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">ds.map(_ + <span class="number">1</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h1 id="3-DataFrame-API"><a href="#3-DataFrame-API" class="headerlink" title="3.DataFrame API"></a>3.DataFrame API</h1><h2 id="3-1-Actions"><a href="#3-1-Actions" class="headerlink" title="3.1 Actions"></a>3.1 Actions</h2><p><strong>collect</strong></p>
<p>返回<code>DataFrame</code>中的所有<code>Row</code>数组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rows = df.collect()</span><br><span class="line">rows.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>collectAsList</strong></p>
<p>返回包含<code>Row</code>的Java列表</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rows = df.collectAsList()</span><br><span class="line">println(rows)</span><br></pre></td></tr></table></figure>

<p><strong>count</strong></p>
<p>返回<code>DataFrame</code>中<code>Row</code>的总行数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> count = df.count</span><br><span class="line">println(<span class="string">&quot;count: &quot;</span> + count)</span><br></pre></td></tr></table></figure>

<p><strong>describe</strong>(cols: String*)</p>
<p>计算数值列的统计信息，包括<strong>总数</strong>、<strong>均值</strong>、<strong>标准差</strong>、<strong>最小值</strong>和<strong>最大值</strong>。如果未给定任何列，则此函数计算所有数值列的统计信息</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">df.describe(<span class="string">&quot;age&quot;</span>).show</span><br></pre></td></tr></table></figure>

<p><strong>first</strong></p>
<p>返回<code>DataFrame</code>中的第一行，等价<code>head()</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">println(df.first)</span><br></pre></td></tr></table></figure>

<p><strong>head</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">println(df.head)</span><br><span class="line">println(df.head(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p><strong>show</strong></p>
<p>以表格的形式显示<code>DataFrame</code>中的数据</p>
<p>第一参数：显示的行数</p>
<p>第二个参数：是否截断长字符串</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">df.show</span><br><span class="line">df.show(<span class="number">10</span>)</span><br><span class="line">df.show(<span class="number">10</span>, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>

<p><strong>take</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">df.take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>takeAsList</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">&quot;/Users/peidonggao/Desktop/people.json&quot;</span>)</span><br><span class="line">df.takeAsList(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h2 id="3-2-DataFrame基本功能"><a href="#3-2-DataFrame基本功能" class="headerlink" title="3.2 DataFrame基本功能"></a>3.2 DataFrame基本功能</h2><p><strong>cache</strong></p>
<p>对<code>DataFrame</code>持久化，默认持久化策略(MEMORY_AND_DISK)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.cache</span><br></pre></td></tr></table></figure>

<p><strong>columns</strong></p>
<p>以数组的形式返回所有列名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.columns</span><br></pre></td></tr></table></figure>

<p><strong>dtypes</strong></p>
<p>以数组的方式返回所有列名及列类型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.dtypes</span><br></pre></td></tr></table></figure>

<p><strong>explain</strong></p>
<p>将计划(逻辑和物理)打印到控制台以进行调试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(df.explain(true))</span><br></pre></td></tr></table></figure>

<p><strong>isLocal</strong></p>
<p>如果collect和take方法可以在本地运行(没有任何spark执行器)，则返回true</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(df.isLocal)</span><br></pre></td></tr></table></figure>

<p><strong>persist</strong>(newLevel: StorageLevel)</p>
<p>使用给定的StorageLevel对<code>DataFrame</code>进行持久化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.persist(StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure>

<p><strong>printSchema</strong></p>
<p>以友好的树形结构将schema打印到控制台</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema</span><br></pre></td></tr></table></figure>

<p><strong>schema</strong></p>
<p>返回<code>DataFrame</code>的schema</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.schema</span><br></pre></td></tr></table></figure>

<p><strong>toDF</strong>(colNames: String*)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">&quot;/Users/peidonggao/Desktop/users.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> users = textFile.map(x =&gt; &#123;</span><br><span class="line"><span class="keyword">val</span> user = x.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">(user(<span class="number">0</span>), user(<span class="number">1</span>), user(<span class="number">2</span>))</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = users.toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.show</span><br></pre></td></tr></table></figure>

<p><strong>unpersist</strong>(blocking: Boolean)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.unpersist(<span class="literal">true</span>)</span><br></pre></td></tr></table></figure>

<h2 id="3-3-查询功能"><a href="#3-3-查询功能" class="headerlink" title="3.3 查询功能"></a>3.3 查询功能</h2><p><strong>select</strong>(cols: Column)</p>
<p>查询指定的列数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;colA&quot;</span>, <span class="string">&quot;colB&quot;</span>)</span><br><span class="line">df.select($<span class="string">&quot;colA&quot;</span>, $<span class="string">&quot;colB&quot;</span>)</span><br><span class="line">df.select($<span class="string">&quot;colA&quot;</span>, $<span class="string">&quot;colB&quot;</span> + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><strong>where</strong>(condition: Column)</p>
<p>条件过滤</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).where($<span class="string">&quot;name&quot;</span>.equalTo(<span class="string">&quot;Justin&quot;</span>) and $<span class="string">&quot;age&quot;</span> &gt; <span class="number">10</span>).show</span><br></pre></td></tr></table></figure>

<p><strong>distinct</strong></p>
<p>去重</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.distinct</span><br></pre></td></tr></table></figure>

<p><strong>drop</strong>(col: Column)</p>
<p>返回一个删除了列的新<code>DataFrame</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newDF = df.drop(<span class="string">&quot;age&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>except</strong>(other: DataFrame)</p>
<p>返回一个新的<code>DataFrame</code>，该<code>DataFrame</code>为两个<code>DataFrame</code>的差集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.except(newDF)</span><br></pre></td></tr></table></figure>

<p><strong>explode</strong></p>
<p>返回一个新的<code>DataFrame</code>，其中一列已被提供的函数扩展为零行或多行</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.explode(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;newName&quot;</span>)&#123;name: <span class="type">String</span> =&gt; name.split(<span class="string">&quot; &quot;</span>)&#125;.show</span><br></pre></td></tr></table></figure>

<p><strong>filter</strong>(condition: Column)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The following are equivalent:</span></span><br><span class="line">peopleDf.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">15</span>)</span><br><span class="line">peopleDf.where($<span class="string">&quot;age&quot;</span> &gt; <span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<p><strong>groupBy</strong>(cols: Column*)</p>
<p>使用指定的列对<code>DataFrame</code>进行分组，以便我们可以对它们运行聚合</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Compute the average for all numeric columns grouped by department.</span></span><br><span class="line">df.groupBy(<span class="string">&quot;department&quot;</span>).avg()</span><br></pre></td></tr></table></figure>

<p><strong>intersect</strong>(other: DataFrame)</p>
<p>返回两个<code>DataFrame</code>的交集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.intersect(newDF).show</span><br></pre></td></tr></table></figure>

<p><strong>join</strong>(right: DataFrame, joinExprs: Column, joinType: String)</p>
<p>joinType：inner outer, left_outer, right_outer, leftsemi</p>
<p>使用给定的连接表达式连接两个<code>DataFrame</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Scala:</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">df1.join(df2, $<span class="string">&quot;df1Key&quot;</span> === $<span class="string">&quot;df2Key&quot;</span>, <span class="string">&quot;outer&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Java:</span></span><br><span class="line"><span class="keyword">import</span> static org.apache.spark.sql.functions.*;</span><br><span class="line">df1.join(df2, col(<span class="string">&quot;df1Key&quot;</span>).equalTo(col(<span class="string">&quot;df2Key&quot;</span>)), <span class="string">&quot;outer&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>limit</strong>(n: Int)</p>
<p>获取前n行并返回新的<code>DataFrame</code>，与head()区别在于head()返回一个数组，limit()返回一个新的<code>DataFrame</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.limit(<span class="number">10</span>).show</span><br></pre></td></tr></table></figure>

<p><strong>sort</strong>(sortExprs: Column*)</p>
<p>根据给定的表达式返回一个排序后的<code>DataFrame</code>,等价与orderBy()</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sort($<span class="string">&quot;col1&quot;</span>, $<span class="string">&quot;col2&quot;</span>.desc).show</span><br></pre></td></tr></table></figure>

<p><strong>agg</strong>(expr: Column, exprs: Column*)</p>
<p>在不分组的情况下，对<code>DataFrame</code>的数据进行聚合</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">df.agg(max($<span class="string">&quot;age&quot;</span>),min($<span class="string">&quot;age&quot;</span>),avg($<span class="string">&quot;age&quot;</span>)).show</span><br></pre></td></tr></table></figure>

<h2 id="3-4-输出操作"><a href="#3-4-输出操作" class="headerlink" title="3.4 输出操作"></a>3.4 输出操作</h2><p><strong>write</strong></p>
<p>将<code>DataFrame</code>的内容保存到外部存储器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">&quot;/Users/peidonggao/Desktop/users.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> users = textFile.map(x =&gt; &#123;</span><br><span class="line"><span class="keyword">val</span> user = x.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">(user(<span class="number">0</span>), user(<span class="number">1</span>), user(<span class="number">2</span>))</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = users.toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> writer = df.write</span><br><span class="line">writer.save(<span class="string">&quot;/Users/peidonggao/Desktop&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="3-5-RDD操作"><a href="#3-5-RDD操作" class="headerlink" title="3.5 RDD操作"></a>3.5 RDD操作</h2><p><strong>coalesce</strong></p>
<p>返回具有<code>numpartitions</code>分区的新<code>DataFrame</code>，此操作会产生窄依赖。例如，如果从1000个分区转到100个分区，则不会出现shuffle</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.coalesce(10)</span><br></pre></td></tr></table></figure>

<p><strong>flatMap</strong></p>
<p>返回一个新的RDD，flatMap中的func将作用于<code>DataFrame</code>中的所有<code>Row</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = df.flatMap(x =&gt; &#123;</span><br><span class="line">	<span class="type">List</span>(x.get(<span class="number">0</span>),x.get(<span class="number">1</span>),x.get(<span class="number">2</span>))</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">result.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>foreach</strong></p>
<p>应用foreach中的func到<code>DataFrame</code>中的每一个<code>Row</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>foreachPartition</strong></p>
<p>应用foreach中的func到<code>DataFrame</code>中的每一个分区</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.foreachPartition(it =&gt; &#123;</span><br><span class="line">	<span class="keyword">while</span>(it.hasNext) &#123;</span><br><span class="line">		println(<span class="string">&quot;row: &quot;</span> + it.next().toString)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p><strong>rdd</strong></p>
<p>将<code>DataFrame</code>的内容表示为<code>Row</code>的RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.rdd</span><br></pre></td></tr></table></figure>

<h1 id="4-DataFrame与RDD互转换"><a href="#4-DataFrame与RDD互转换" class="headerlink" title="4.DataFrame与RDD互转换"></a>4.DataFrame与RDD互转换</h1><p>Spark SQL支持将现有<strong>RDD</strong>转换为<strong>DataFrame</strong>的两种不同方法</p>
<ul>
<li><p>基于反射推断</p>
<p>基于反射的方法，在已知schema时，可以编写出更简洁的spark应用</p>
</li>
<li><p>基于编程接口构造schema</p>
<p>该接口允许你构造一个schema，然后将其应用到现有的RDD</p>
</li>
</ul>
<h2 id="4-1-基于反射推断"><a href="#4-1-基于反射推断" class="headerlink" title="4.1 基于反射推断"></a>4.1 基于反射推断</h2><p>Spark SQL中的scala接口支持将包含case class的RDD自动转换为DataFrame</p>
<p>case class用于定义schema，使用反射读取case class中的参数名称，并反射成DataFrame的列</p>
<p>case class可以嵌套或包含复杂类型，如序列和数组</p>
<p><font color=red>RDD可以隐式转换为DataFrame，然后注册成表，并可使用SQL语句使用访问操作</font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义case class</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">    .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">  <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line">  <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="comment">//创建Person对象的RDD</span></span><br><span class="line">  <span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">&quot;/Users/peidonggao/Desktop/people.txt&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>)).map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt))</span><br><span class="line">  <span class="comment">//RDD转换为DataFrame</span></span><br><span class="line">  <span class="keyword">val</span> df = peopleRDD.toDF()</span><br><span class="line">  <span class="comment">//DataFrame注册为表</span></span><br><span class="line">  df.registerTempTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line">  <span class="comment">//使用SQLContext提供的SQL方法来运行SQL语句</span></span><br><span class="line">  <span class="keyword">val</span> peoples = sqlContext.sql(<span class="string">&quot;select name, age from people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span>)</span><br><span class="line">  peoples.show</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><font color=red>注：case class要定义在全局作用域</font></p>
<h2 id="4-2-基于编程接口构造schema"><a href="#4-2-基于编程接口构造schema" class="headerlink" title="4.2 基于编程接口构造schema"></a>4.2 基于编程接口构造schema</h2><p>如果不能提前定义case class，则可以通过三个步骤以编程方式创建<code>DataFrame</code></p>
<ol>
<li>从原始RDD创建基于<code>Row</code>的RDD</li>
<li>使用<code>StructType</code>定义步骤1创建的RDD的schema</li>
<li>使用SQLContext的<code>createDataFrame()</code>对RDD进行转换</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">&quot;/Users/peidonggao/Desktop/people.txt&quot;</span>)</span><br><span class="line"><span class="comment">//基于字符串编码的schema</span></span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">&quot;name age&quot;</span></span><br><span class="line"><span class="comment">// Import Row.</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="comment">// Import Spark SQL data types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructType</span>,<span class="type">StructField</span>,<span class="type">StringType</span>&#125;</span><br><span class="line"><span class="comment">//根据字符串编码的schema创建数据结构</span></span><br><span class="line"><span class="keyword">val</span> schema =</span><br><span class="line">  <span class="type">StructType</span>(</span><br><span class="line">    schemaString.split(<span class="string">&quot; &quot;</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将RDD数据转换为Row</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = peopleRDD.map(_.split(<span class="string">&quot;,&quot;</span>)).map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim))</span><br><span class="line"><span class="comment">//将schema应用于RDD</span></span><br><span class="line"><span class="keyword">val</span> df = sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line"><span class="comment">//DataFrame注册为表</span></span><br><span class="line">df.registerTempTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"><span class="comment">//使用SQLContext提供的SQL方法来运行SQL语句</span></span><br><span class="line"><span class="keyword">val</span> results = sqlContext.sql(<span class="string">&quot;select name from people&quot;</span>)</span><br><span class="line">results.show</span><br></pre></td></tr></table></figure>

<h1 id="5-数据源"><a href="#5-数据源" class="headerlink" title="5.数据源"></a>5.数据源</h1><p>Spark SQL支持通过DataFrame接口对各种数据源进行操作</p>
<p>DataFrame可以作为普通RDD操作，也可以注册为临时表</p>
<h2 id="5-1-通用加载-保存功能"><a href="#5-1-通用加载-保存功能" class="headerlink" title="5.1 通用加载/保存功能"></a>5.1 通用加载/保存功能</h2><p>Spark中，默认加载数据源为parquet文件类型，除非手动指定<code>spark.sql.sources.default</code>属性</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>手动指定配置</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.format(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesAndAges.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>直接对文件运行SQL</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.sql(<span class="string">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>存储模式</strong></p>
<p>保存操作可以选择采用保存模式，该模式指定如何处理现有数据(如果存在)</p>
<p>重要的是要认识到这些保存模式不使用任何锁定，也不是原子的</p>
<p>在执行覆盖时，数据将在写入新数据之前被删除</p>
<table>
<thead>
<tr>
<th>保存模式</th>
<th>级别</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>将<code>DataFrame</code>保存到数据源时，如果数据已存在，则应引发异常</td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>将<code>DataFrame</code>保存到数据源时，如果数据/表已经存在，则<code>DataFrame</code>的内容将被追加到现有数据中</td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>覆盖模式是指在将<code>DataFrame</code>保存到数据源时，如果数据/表已经存在，则现有数据将被<code>DataFrame</code>的内容覆盖</td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>忽略模式是指在将<code>DataFrame</code>保存到数据源时，如果数据已经存在，则保存操作不会保存<code>DataFrame</code>的内容，也不会更改现有数据</td>
</tr>
</tbody></table>
<p><strong>代码演示说明</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.load(<span class="string">&quot;/Users/peidonggao/Desktop/users.parquet&quot;</span>)</span><br><span class="line">df.write.format(<span class="string">&quot;json&quot;</span>).mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).save(<span class="string">&quot;/Users/peidonggao/Desktop/json&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="5-2-Parquet文件"><a href="#5-2-Parquet文件" class="headerlink" title="5.2 Parquet文件"></a>5.2 Parquet文件</h2><p>Parquet是需要其他数据处理系统支持的列式格式类型</p>
<p>Spark SQL支持读取和写入Parquet文件时，自动维护原始数据的schema</p>
<p>对Parquet文件进行写入时，考虑到兼容性，所有列都自动转换为可以为空</p>
<p><strong>数据加载</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> parquetFile = sqlContext.read.parquet(<span class="string">&quot;/Users/peidonggao/Desktop/users.parquet&quot;</span>)</span><br><span class="line"><span class="comment">//Parquet files can also be registered as tables and then used in SQL statements.</span></span><br><span class="line">parquetFile.registerTempTable(<span class="string">&quot;parquetFile&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">&quot;SELECT name FROM parquetFile&quot;</span>)</span><br><span class="line">teenagers.show</span><br></pre></td></tr></table></figure>

<p><strong>分区推断</strong></p>
<p>表分区是类似hive的系统中常用的优化方法</p>
<p>在分区表中，数据通常存储在不同的目录中，分区列值编码在每个分区目录的路径中</p>
<p>Parquet数据源能够能够自动发现和推断分区信息</p>
<p>创建分区</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/root/parquet/gender=male/country=CN</span><br></pre></td></tr></table></figure>

<p>上传数据到该目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put ~/users.parquet /user/root/parquet/gender=male/country=CN</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.parquet(<span class="string">&quot;hdfs://192.168.198.128:9000/user/root/parquet/gender=male/country=CN/users.parquet&quot;</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p><strong>Parquet合并元数据</strong></p>
<p>与ProtocolBuffer、Avro和Thrift一样，Parquet也支持schema进化</p>
<p>用户可以从一个简单的schema开始，并根据需要逐步向该模式添加更多列</p>
<p>这样，用户最终可能会得到多个具有不同但相互兼容模式的Parquet文件</p>
<p><font color=red>由于模式合并是一个相对昂贵的操作，并且在大多数情况下不是必需的，所以我们默认从1.5.0开始关闭它</font></p>
<p>你可以通过</p>
<ol>
<li>在读取Parquet文件时，将数据源选项mergeschema设置为true</li>
<li>将全局SQL选项<code>spark.sql.parquet.mergeschema</code>设置为true</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">&quot;single&quot;</span>, <span class="string">&quot;double&quot;</span>)</span><br><span class="line">df1.write.parquet(<span class="string">&quot;hdfs://192.168.198.128:9000/user/root/data/test_table/key=1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">&quot;single&quot;</span>, <span class="string">&quot;triple&quot;</span>)</span><br><span class="line">df2.write.parquet(<span class="string">&quot;hdfs://192.168.198.128:9000/user/root/data/test_table/key=2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df3 = sqlContext.read.option(<span class="string">&quot;mergeSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).parquet(<span class="string">&quot;hdfs://192.168.198.128:9000/user/root/data/test_table&quot;</span>)</span><br><span class="line">df3.printSchema()</span><br></pre></td></tr></table></figure>

<h2 id="5-3-JSON数据集源"><a href="#5-3-JSON数据集源" class="headerlink" title="5.3 JSON数据集源"></a>5.3 JSON数据集源</h2><p>Spark SQL可以自动推断JSON数据集的schema并将其作为<code>DataFrame</code>加载</p>
<p>这种转换可以通过<code>SQLContext.read.json()</code>加载字符串形式的RDD或JSON文件</p>
<p><font color=red>请注意，作为JSON文件提供的文件不是典型的JSON文件</font></p>
<p><font color=red>每一行必须包含一个独立的、自包含的有效JSON对象。因此，常规的多行JSON文件通常会失败</font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> stuDF = sqlContext.read.json(<span class="string">&quot;C:\\Users\\MAX\\Desktop\\students.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> scoreDF = sqlContext.read.json(<span class="string">&quot;C:\\Users\\MAX\\Desktop\\scores.json&quot;</span>)</span><br><span class="line"><span class="comment">//注册表</span></span><br><span class="line">stuDF.registerTempTable(<span class="string">&quot;student&quot;</span>)</span><br><span class="line">scoreDF.registerTempTable(<span class="string">&quot;score&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stu_scoreDF = sqlContext.sql(<span class="string">&quot;select stu.name,sc.score from student stu join score sc on stu.id=sc.id&quot;</span>)</span><br><span class="line">stu_scoreDF.show</span><br></pre></td></tr></table></figure>

<p><strong>复杂JSON格式处理</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;spark sql&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> bookDF = sqlContext.read.json(<span class="string">&quot;C:\\Users\\MAX\\Desktop\\book_info.json&quot;</span>)</span><br><span class="line">bookDF.registerTempTable(<span class="string">&quot;book&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> result = sqlContext.sql(<span class="string">&quot;select bookId,bookName,price,author.name,author.age,author.description from book&quot;</span>)</span><br><span class="line">result.printSchema</span><br></pre></td></tr></table></figure>

<h2 id="5-4-Hive数据源"><a href="#5-4-Hive数据源" class="headerlink" title="5.4 Hive数据源"></a>5.4 Hive数据源</h2><p>Spark SQL也支持对Hive中的数据进行读写操作。但是，由于hive具有大量依赖项，因此它不包含在默认的Spark程序集中</p>
<ol>
<li><p>配置HIve metaStore Service</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://master:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>开启Hive metaStore Service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure></li>
<li><p>拷贝Hive conf/hive-site.xml到Spark conf目录下</p>
</li>
<li><p>拷贝mysql-connector-java-5.1.27-bin.jar到Spark lib目录下</p>
</li>
<li><p>编写脚本</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;hive dataSource&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> hiveContext = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line">hiveContext.sql(<span class="string">&quot;drop table if exists student_info&quot;</span>)</span><br><span class="line">hiveContext.sql(<span class="string">&quot;create table if not exists student_info(name string, age int) &quot;</span></span><br><span class="line">			  + <span class="string">&quot;ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27; &#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">&quot;drop table if exists student_score&quot;</span>)</span><br><span class="line">hiveContext.sql(<span class="string">&quot;create table if not exists student_score(name string, score int) &quot;</span></span><br><span class="line">  + <span class="string">&quot;ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27; &#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//load student data</span></span><br><span class="line">hiveContext.sql(<span class="string">&quot;LOAD DATA INPATH &quot;</span></span><br><span class="line">			  + <span class="string">&quot;&#x27;/user/root/data/student_info.txt&#x27; &quot;</span></span><br><span class="line">			  + <span class="string">&quot;INTO TABLE student_info&quot;</span>)</span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">&quot;LOAD DATA INPATH &quot;</span></span><br><span class="line">  + <span class="string">&quot;&#x27;/user/root/data/student_score.txt&#x27; &quot;</span></span><br><span class="line">  + <span class="string">&quot;INTO TABLE student_score&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//join</span></span><br><span class="line"><span class="keyword">val</span> resultDF = hiveContext.sql(<span class="string">&quot;select si.name,si.age,ss.score from &quot;</span></span><br><span class="line">						  + <span class="string">&quot;student_info si join student_score ss &quot;</span></span><br><span class="line">						  + <span class="string">&quot;on si.name = ss.name &quot;</span>)</span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">&quot;drop table if exists t_students&quot;</span>)</span><br><span class="line">resultDF.write.saveAsTable(<span class="string">&quot;t_students&quot;</span>)</span><br><span class="line">resultDF.show</span><br></pre></td></tr></table></figure>

<p><strong>spark-submit</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line"> --class SparkSQLHiveTest \</span><br><span class="line"> --master spark://master:7077 \</span><br><span class="line"> --executor-memory 2G \</span><br><span class="line"> --total-executor-cores 2 \</span><br><span class="line"> --files /opt/modules/apache-hive-1.2.1-bin/conf/hive-site.xml \</span><br><span class="line"> --jars /opt/modules/spark-1.6.3-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar,/opt/modules/spark-1.6.3-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar,/opt/modules/spark-1.6.3-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar,/opt/modules/spark-1.6.3-bin-hadoop2.6/lib/mysql-connector-java-5.1.27-bin.jar \</span><br><span class="line">/opt/modules/spark-1.6.3-bin-hadoop2.6/scala/spark-demo4.jar</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="5-5-JDBC数据源"><a href="#5-5-JDBC数据源" class="headerlink" title="5.5 JDBC数据源"></a>5.5 JDBC数据源</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> content = sqlContext.read</span><br><span class="line">              .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">              .options(<span class="type">Map</span>(</span><br><span class="line">                <span class="string">&quot;driver&quot;</span> -&gt; <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>,</span><br><span class="line">                <span class="string">&quot;url&quot;</span> -&gt; <span class="string">&quot;jdbc:mysql://localhost:3306/demo&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dbtable&quot;</span> -&gt; <span class="string">&quot;t_user&quot;</span>,</span><br><span class="line">                <span class="string">&quot;user&quot;</span> -&gt; <span class="string">&quot;root&quot;</span>,</span><br><span class="line">                <span class="string">&quot;password&quot;</span> -&gt; <span class="string">&quot;111111&quot;</span></span><br><span class="line">              )).load</span><br><span class="line"></span><br><span class="line">content.show</span><br></pre></td></tr></table></figure>

<h1 id="6-性能优化"><a href="#6-性能优化" class="headerlink" title="6.性能优化"></a>6.性能优化</h1><h2 id="6-1-数据缓存"><a href="#6-1-数据缓存" class="headerlink" title="6.1 数据缓存"></a>6.1 数据缓存</h2><p>Spark SQL可以通过调用<code>sqlContext.cacheTable(&quot;tableName&quot;) </code>或<code>dataframe.cache()</code>将列格式的表缓存到内存中</p>
<p>之后，Spark SQL只需扫描所需的列，并自动调整压缩，以最小化内容使用和降低GC压力</p>
<p>此外，可以通过调用<code>sqlContext.uncachetable(“tablename”)</code>从内存中删除表</p>
<p>内存缓存的配置可以使用<code>sqlContext</code>的<code>setconf</code>方法完成</p>
<table>
<thead>
<tr>
<th>属性名</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.inMemoryColumnarStorage.compressed</td>
<td>true</td>
<td>当设置为true，Spark SQL将根据数据统计信息，为每列自动选择一种压缩编码器</td>
</tr>
<tr>
<td>spark.sql.inMemoryColumnarStorage.batchSize</td>
<td>10000</td>
<td>缓存批处理大小， 较大的批处理可以提高内存利用率和压缩率，但同时也会带来 OOM(Out Of Memory)的风险</td>
</tr>
<tr>
<td>spark.sql.autoBroadcastJoinThreshold</td>
<td>10485760 (10 MB)</td>
<td>配置执行broadcast join的最大字节表的阈值，小于该阈值的表在执行join操作时会使用broadcast join来优化原有的join操作</td>
</tr>
<tr>
<td>spark.sql.tungsten.enabled</td>
<td>true</td>
<td>当设置为true，则使用优化后的钨丝物理执行计划，底层会显式管理内存并动态生成字节码</td>
</tr>
<tr>
<td>spark.sql.shuffle.partitions</td>
<td>200</td>
<td>配置在进行join或aggregation操作时的分区数</td>
</tr>
</tbody></table>
</div><div class="tags"><a href="/tags/Spark/"><i class="fa fa-tag"></i>Spark</a></div><div class="post-nav"><a class="pre" href="/2021/08/11/Scala/">Scala</a><a class="next" href="/2021/08/11/Spark%20Streaming/">Spark Streaming</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/FastDFS/">FastDFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JAVA-%E9%A1%B9%E7%9B%AE/">JAVA-项目</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JAVA%E5%9F%BA%E7%A1%80/">JAVA基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JVM/">JVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kafaka/">Kafaka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MyBatis/">MyBatis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Redis/">Redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring-Boot/">Spring Boot</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringMVC/">SpringMVC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7/">版本控制工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">计算机网络</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/JAVA/" style="font-size: 15px;">JAVA</a> <a href="/tags/FastDFS/" style="font-size: 15px;">FastDFS</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/JAVA-WEB/" style="font-size: 15px;">JAVA-WEB</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/SVN/" style="font-size: 15px;">SVN</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/tags/SSL/" style="font-size: 15px;">SSL</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/MyBatis/" style="font-size: 15px;">MyBatis</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Spring-Boot/" style="font-size: 15px;">Spring Boot</a> <a href="/tags/%E9%A1%B9%E7%9B%AE/" style="font-size: 15px;">项目</a> <a href="/tags/SpringMVC/" style="font-size: 15px;">SpringMVC</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/%E5%BF%83%E6%83%85/" style="font-size: 15px;">心情</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2050/08/09/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8F%90%E4%BA%A4/">👋 Hi!</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/5.%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/">JAVA集合框架</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/FastDFS%E5%85%A5%E9%97%A8/">FastDFS</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/6.%E6%96%87%E4%BB%B6%E4%B8%8EIO/">文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/7.JDBC/">JAVA-JDBC</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/Git/">Git</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/JavaWeb(%E4%BA%8C)/">Http之HttpServlet</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/Redis%E9%AB%98%E7%BA%A7/">Redis高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/Docker/">Docker</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/11/MyBatis/">MyBatis</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">云起迎风燕.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>